{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import argparse\n",
    "import numpy as np\n",
    "from transformers import HfArgumentParser, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import Dataset\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_length, context_key=\"answerEntity\",\n",
    "                 tokenizer_truncation=\"only_first\", graph_only=False):\n",
    "        super(QuestionAnswerDataset).__init__()\n",
    "\n",
    "        self.questions = df.question.values\n",
    "        self.contexts = df[context_key].values\n",
    "        self.labels = torch.tensor(df.label.values, dtype=torch.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.graph_only = graph_only\n",
    "        if graph_only:\n",
    "            self.tokenized_input = [tokenizer.encode_plus(y,\n",
    "                              max_length=self.max_length,\n",
    "                              padding=\"max_length\",\n",
    "                              truncation=tokenizer_truncation,\n",
    "                              return_tensors=\"pt\") \\\n",
    "                              for y in self.contexts]\n",
    "        else:\n",
    "            self.tokenized_input = [tokenizer.encode_plus(x, y,\n",
    "                              max_length=self.max_length,\n",
    "                              padding=\"max_length\",\n",
    "                              truncation=tokenizer_truncation,\n",
    "                              return_tensors=\"pt\", ) \\\n",
    "                          for x, y in zip(self.questions,\n",
    "                                          self.contexts)]\n",
    "        assert len(self.questions) == len(self.contexts) == len(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "                \"input_ids\": self.tokenized_input[idx][\"input_ids\"][0],\n",
    "                \"attention_mask\" : self.tokenized_input[idx][\"attention_mask\"][0],\n",
    "                \"labels\": self.labels[idx]}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
